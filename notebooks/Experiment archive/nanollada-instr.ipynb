{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11731234,"sourceType":"datasetVersion","datasetId":7364261}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Custom Dataset class for ThaiSum with instruction prompting\nclass ThaiSumDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_input_length=512, max_target_length=128):\n        self.texts = dataframe['body'].tolist()\n        self.summaries = dataframe['summary'].tolist()\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_target_length = max_target_length\n        self.instruction = \"สรุปข้อความนี้เป็นภาษาไทย:\"  # Thai instruction\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        summary = str(self.summaries[idx])\n\n        # Prepend instruction to input text and append summary as target\n        input_text = f\"{self.instruction} {text}\"\n        target_text = f\"{summary}\"\n\n        # Tokenize input and target\n        input_encoding = self.tokenizer(\n            input_text,\n            max_length=self.max_input_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        target_encoding = self.tokenizer(\n            target_text,\n            max_length=self.max_target_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Combine input and target for CLM (shifted labels)\n        input_ids = torch.cat([input_encoding['input_ids'].squeeze(), target_encoding['input_ids'].squeeze()], dim=0)\n        labels = input_ids.clone()\n        labels[:input_encoding['input_ids'].shape[1]] = -100  # Ignore loss for input part\n\n        return {\n            'input_ids': input_ids,\n            'labels': labels\n        }\n\n# q_t_given_0 function from LLADA (adapted for input)\ndef q_t_given_0(input_ids, mask_token_id, t, N, tokenizer):\n    \"\"\"\n    Remask tokens according to q_{t|0}: mask each token with probability s = t/N.\n    \"\"\"\n    s = t / N\n    special_tokens_mask = (input_ids == tokenizer.pad_token_id)  # Only protect pad tokens\n    rand_mask = torch.bernoulli(torch.full(input_ids.shape, s)).bool().to(input_ids.device)\n    mask_positions = rand_mask & ~special_tokens_mask\n\n    masked_input = input_ids.clone()\n    masked_input[mask_positions] = mask_token_id\n    return masked_input, mask_positions\n\n# Training step with LLADA diffusion and mixed precision\ndef training_step(model, tokenizer, batch, N, scaler):\n    input_ids = batch['input_ids'].to(model.device)\n    labels = batch['labels'].to(model.device)\n\n    # Apply LLADA diffusion to input (up to input part)\n    input_length = input_ids.shape[0] - labels.ne(-100).sum()  # Length of input part\n    input_ids_input = input_ids[:input_length]\n    t = random.randint(1, N)\n    masked_input_ids, _ = q_t_given_0(input_ids_input, tokenizer.pad_token_id, t, N, tokenizer)\n    input_ids[:input_length] = masked_input_ids\n\n    # Mixed precision forward pass\n    with autocast(device_type='cuda'):\n        outputs = model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n\n    # Scale loss and backpropagate\n    scaler.scale(loss).backward()\n    return loss\n\n# Training loop with mixed precision\ndef train_summarization_llada(model, tokenizer, dataset, optimizer, epochs=3, N=10, batch_size=8):\n    model.train()\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    scaler = GradScaler()\n\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            optimizer.zero_grad()\n            loss = training_step(model, tokenizer, batch, N, scaler)\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(dataloader)\n        print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}\")\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load dataset\n    df = pd.read_csv('/kaggle/input/thaisum-train-10000-1024-nlpfinal/train-10000-1024.csv')\n\n    # Initialize tokenizer and model\n    model_name = 'facebook/xglm-564M'  # ~564M parameters, supports Thai\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n\n    # Prepare dataset\n    dataset = ThaiSumDataset(df, tokenizer, max_input_length=1024, max_target_length=256)\n\n    # Set hyperparameters\n    epochs = 1\n    batch_size = 2\n    N = 10  # For q_t_given_0\n    learning_rate = 2e-5\n\n    # Initialize optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    # Train the model\n    train_summarization_llada(model, tokenizer, dataset, optimizer, epochs=epochs, N=N, batch_size=batch_size)\n\n    # Save the model\n    model.save_pretrained('./thai_summarization_llada_xglm564m')\n    tokenizer.save_pretrained('./thai_summarization_llada_xglm564m')\n    print(\"Model and tokenizer saved to './thai_summarization_llada_xglm564m'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T13:55:33.815469Z","iopub.execute_input":"2025-05-08T13:55:33.815631Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5938e375c146c198a475ca3ca1eb20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547a8a3429d24582bf25a077dcfadf31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f4288d78ad4104ad828dce7387377c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/276 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a65b940dd0e4557838a5aed7db092af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588c3291ec434ef7b25b6113acd8f673"}},"metadata":{}},{"name":"stderr","text":"2025-05-08 13:55:58.483575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746712558.712118      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746712558.777264      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68683a7f39034c3384467c55fae7b77a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c82ab62f711483b9fedc51d2e63d7d5"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch.nn.functional import softmax\n\ndef infer_llada(input_text, model, tokenizer, L=128, N=10):\n    # Move model to device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    # Prepend instruction to input text\n    instruction = \"สรุปข้อความนี้เป็นภาษาไทย:\"\n    prompt = f\"{instruction} {input_text}\"\n\n    # Tokenize input text with instruction\n    input_encoding = tokenizer(\n        prompt,\n        max_length=512,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    ).to(device)\n\n    # Initialize fully masked sequence of length L\n    masked_ids = torch.full((1, L), tokenizer.pad_token_id, dtype=torch.long).to(device)\n    r_t = masked_ids.clone()\n\n    # Sampling steps\n    for t in range(N, 0, -1):\n        s = t / N\n\n        # Predict next tokens\n        with torch.no_grad():\n            outputs = model(input_ids=input_encoding['input_ids'], labels=r_t)\n            logits = outputs.logits[:, -1, :]  # Take logits for the last token position\n            probs = softmax(logits, dim=-1)\n            confidences, predicted_ids = torch.max(probs, dim=-1)\n\n        # Update r_t with predicted tokens\n        r_0 = r_t.clone()\n        c = torch.ones_like(r_0, dtype=torch.float).to(device)  # Confidence scores\n\n        for i in range(L):\n            if r_t[0, i] != tokenizer.pad_token_id:  # If not masked\n                r_0[0, i] = r_t[0, i]\n                c[0, i] = 1.0\n            else:\n                r_0[0, i] = predicted_ids[0]\n                c[0, i] = confidences[0].item()\n\n        # Calculate number of unmasked tokens\n        n_un = int(L * (1 - s))\n\n        # Remask the n_un least confident positions\n        if n_un > 0:\n            _, lowest_conf_indices = torch.topk(c[0], n_un, largest=False)\n            for idx in lowest_conf_indices:\n                r_0[0, idx] = tokenizer.pad_token_id\n\n        r_t = r_0.clone()\n\n    # Final generation\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids=input_encoding['input_ids'],\n            max_length=L,\n            num_beams=1,\n            early_stopping=True,\n            pad_token_id=tokenizer.pad_token_id\n        )\n    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    return summary\n\ndef main():\n    # Load model and tokenizer\n    model_path = './thai_summarization_llada_qwen05b'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n\n    # Example input text\n    input_text = \"นี่คือตัวอย่างข้อความภาษาไทยที่ยาวมากเกี่ยวกับข่าวประจำวัน ซึ่งมีรายละเอียดเกี่ยวกับเหตุการณ์สำคัญในประเทศไทย เช่น การเมือง เศรษฐกิจ และวัฒนธรรม\"\n\n    # Perform inference\n    summary = infer_llada(input_text, model, tokenizer, L=128, N=10)\n    print(\"Summary:\", summary)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}