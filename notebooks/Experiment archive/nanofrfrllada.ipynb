{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11731234,"sourceType":"datasetVersion","datasetId":7364261}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Custom Dataset class for ThaiSum\nclass ThaiSumDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_input_length=1024, max_target_length=256):\n        self.texts = dataframe['body'].tolist()\n        self.summaries = dataframe['summary'].tolist()\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        summary = str(self.summaries[idx])\n\n        # Tokenize input (body)\n        input_encoding = self.tokenizer(\n            text,\n            max_length=self.max_input_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        # Tokenize target (summary)\n        target_encoding = self.tokenizer(\n            summary,\n            max_length=self.max_target_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': input_encoding['input_ids'].squeeze(),\n            'attention_mask': input_encoding['attention_mask'].squeeze(),\n            'labels': target_encoding['input_ids'].squeeze()\n        }\n\n# q_t_given_0 function from LLADA\ndef q_t_given_0(input_ids, mask_token_id, t, N, tokenizer):\n    \"\"\"\n    Remask tokens according to q_{t|0}: mask each token with probability s = t/N.\n    \"\"\"\n    s = t / N\n    special_tokens_mask = (input_ids == tokenizer.pad_token_id)  # Only protect pad tokens\n    rand_mask = torch.bernoulli(torch.full(input_ids.shape, s)).bool().to(input_ids.device)\n    mask_positions = rand_mask & ~special_tokens_mask\n\n    masked_input = input_ids.clone()\n    masked_input[mask_positions] = mask_token_id\n    return masked_input, mask_positions\n\n# Training step with LLADA diffusion and mixed precision\ndef training_step(model, tokenizer, batch, N, scaler):\n    input_ids = batch['input_ids'].to(model.device)\n    attention_mask = batch['attention_mask'].to(model.device)\n    labels = batch['labels'].to(model.device)\n\n    # Apply LLADA diffusion to input\n    t = random.randint(1, N)\n    masked_input_ids, _ = q_t_given_0(input_ids, tokenizer.pad_token_id, t, N, tokenizer)\n\n    # Mixed precision forward pass\n    with autocast(device_type='cuda'):\n        outputs = model(\n            input_ids=masked_input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss\n\n    # Scale loss and backpropagate\n    scaler.scale(loss).backward()\n    return loss\n\n# Training loop with mixed precision\ndef train_summarization_llada(model, tokenizer, dataset, optimizer, epochs=3, N=10, batch_size=8):\n    model.train()\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    scaler = GradScaler()\n\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            optimizer.zero_grad()\n            loss = training_step(model, tokenizer, batch, N, scaler)\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(dataloader)\n        print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}\")\n\ndef main():\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load dataset\n    df = pd.read_csv('/kaggle/input/thaisum-train-10000-1024-nlpfinal/train-10000-1024.csv')\n\n    # Initialize tokenizer and model\n    model_name = 'google/mt5-small'  # ~300M parameters, supports Thai\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\n    # Prepare dataset\n    dataset = ThaiSumDataset(df, tokenizer, max_input_length=1024, max_target_length=256)\n\n    # Set hyperparameters\n    epochs = 1\n    batch_size = 2\n    N = 100  # For q_t_given_0\n    learning_rate = 2e-5\n\n    # Initialize optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    # Train the model\n    train_summarization_llada(model, tokenizer, dataset, optimizer, epochs=epochs, N=N, batch_size=batch_size)\n\n    # Save the model\n    model.save_pretrained('./thai_summarization_llada_model_small')\n    tokenizer.save_pretrained('./thai_summarization_llada_model_small')\n    print(\"Model and tokenizer saved to './thai_summarization_llada_model_small'\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T12:32:46.234228Z","iopub.execute_input":"2025-05-08T12:32:46.234786Z","iopub.status.idle":"2025-05-08T12:51:04.506168Z","shell.execute_reply.started":"2025-05-08T12:32:46.234761Z","shell.execute_reply":"2025-05-08T12:51:04.505387Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nEpoch 1/1: 100%|██████████| 5000/5000 [18:06<00:00,  4.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Avg Loss: nan\nModel and tokenizer saved to './thai_summarization_llada_model_small'\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom torch.nn.functional import softmax\n\ndef infer_llada(input_text, model, tokenizer, L=128, N=10):\n    # Move model to device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    # Tokenize input text\n    input_encoding = tokenizer(\n        input_text,\n        max_length=512,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    ).to(device)\n\n    # Initialize fully masked sequence of length L\n    masked_ids = torch.full((1, L), tokenizer.pad_token_id, dtype=torch.long).to(device)\n    r_t = masked_ids.clone()\n\n    # Sampling steps\n    for t in range(N, 0, -1):\n        s = t / N\n\n        # Predict next tokens\n        with torch.no_grad():\n            outputs = model(input_ids=input_encoding['input_ids'], decoder_input_ids=r_t)\n            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n            probs = softmax(logits, dim=-1)\n            confidences, predicted_ids = torch.max(probs, dim=-1)\n\n        # Update r_t with predicted tokens\n        r_0 = r_t.clone()\n        c = torch.ones_like(r_0, dtype=torch.float).to(device)  # Confidence scores\n\n        for i in range(L):\n            if r_t[0, i] != tokenizer.pad_token_id:  # If not masked\n                r_0[0, i] = r_t[0, i]\n                c[0, i] = 1.0\n            else:\n                r_0[0, i] = predicted_ids[0, i]\n                c[0, i] = confidences[0, i]\n\n        # Calculate number of unmasked tokens\n        n_un = int(L * (1 - s))\n\n        # Remask the n_un least confident positions\n        if n_un > 0:\n            _, lowest_conf_indices = torch.topk(c[0], n_un, largest=False)\n            for idx in lowest_conf_indices:\n                r_0[0, idx] = tokenizer.pad_token_id\n\n        r_t = r_0.clone()\n\n    # Final sequence\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids=input_encoding['input_ids'],\n            max_length=L,\n            num_beams=1,\n            early_stopping=True,\n            decoder_start_token_id=tokenizer.pad_token_id\n        )\n    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    return summary\n\ndef main():\n    # Load model and tokenizer\n    model_path = './thai_summarization_llada_model_small'\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n    # Example input text\n    input_text = \"กีเก ซานเชซ ฟลอเรส\\xa0 กุนซือเลือดกระทิงของทีมวัตฟอร์ด\\xa0 เมินประเด็นจุดโทษปัญหาในเกมพรีเมียร์ลีก อังกฤษ นัดที่แตนอาละวาดเปิดบ้านพ่าย คริสตัล พาเลซ 0-1ชี้ทีมของเขาเล่นไม่ดีพอเอง,สำนักข่าวต่างประเทศรายงานวันที่ 27 ก.ย. ว่า กีเก ซานเชซ ฟลอเรส\\xa0 ผู้จัดการทีมชาวสเปน ของ แตนอาละวาด วัตฟอร์ด\\xa0 ยอมรับทีมของเขาเล่นได้ไม่ดีพอเอง ในเกมพรีเมียร์ลีก อังกฤษ นัดเปิดบ้านพ่าย อินทรีผงาด คริสตัล พาเลซ 0-1 เมื่อคืนวันอาทิตย์ที่ผ่านมา,เกมนี้จุดเปลี่ยนมาอยู่ที่การได้จุดโทษในช่วงครึ่งหลังของ คริสตัล พาเลซ ซึ่งไม่ค่อยชัดเจนเท่าไหร่ว่า อัลลัน นียอม นั้นไปทำฟาล์วใส่ วิลฟรีด ซาฮา ในเขตโทษหรือไม่ แต่ผู้ตัดสินก็ชี้เป็นจุดโทษ ซึ่ง โยอัน กาบาย สังหารไม่พลาด และเป็นประตูชัยช่วยให้ คริสตัล พาเลซ เอาชนะ วัตฟอร์ด ไป 1-0 และเป็นการพ่ายแพ้ในบ้านนัดแรกของวัตฟอร์ดในฤดูกาลนี้อีกด้วย,ฟลอเรส กล่าวว่า มันเป็นเรื่องยากในการหยุดเกมรุกของคริสตัล พาเลซ ซึ่งมันอึดอัดจริงๆสำหรับเรา เราเล่นกันได้ไม่ดีนักในตอนที่ได้ครองบอล เราต้องเล่นทางริมเส้นให้มากกว่านี้ เราไม่สามารถหยุดเกมสวนกลับของพวกเขาได้ และแนวรับของเราก็ยืนไม่เป็นระเบียบสักเท่าไหร่ในช่วงครึ่งแรก ส่วนเรื่องจุดโทษการตัดสินใจขั้นสุดท้ายมันอยู่ที่ผู้ตัดสิน ซึ่งมันเป็นการตัดสินใจที่สำคัญ ผมเองก็ไม่รู้ว่าเขาตัดสินถูกหรือเปล่า บางทีมันอาจเป็นจุดที่ตัดสินเกมนี้เลย แต่เราไม่ได้แพ้เกมนี้เพราะจุดโทษ เราแพ้ในวันนี้เพราะเราเล่นไม่ดีและคริสตัล พาเลซ เล่นดีกว่าเรา เราไม่ได้มีฟอร์มการเล่นที่ดีในเกมนี้เลย\"  # Replace with actual input\n\n    # Perform inference\n    summary = infer_llada(input_text, model, tokenizer, L=256, N=10)\n    print(\"Summary:\", summary)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T12:58:55.480450Z","iopub.execute_input":"2025-05-08T12:58:55.480790Z","iopub.status.idle":"2025-05-08T12:58:57.579540Z","shell.execute_reply.started":"2025-05-08T12:58:55.480767Z","shell.execute_reply":"2025-05-08T12:58:57.578944Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Summary: <extra_id_0> กล่าว\n","output_type":"stream"}],"execution_count":5}]}