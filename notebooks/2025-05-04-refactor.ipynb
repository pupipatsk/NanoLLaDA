{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Installation --\n",
    "%pip install --quiet peft datasets lightning ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_PATH: /Users/pupipatsingkhorn/Developer/repositories/NanoLLaDA\n"
     ]
    }
   ],
   "source": [
    "# -- iPython Config --\n",
    "from IPython import get_ipython\n",
    "if \"IPython.extensions.autoreload\" not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
    "else:\n",
    "    get_ipython().run_line_magic(\"reload_ext\", \"autoreload\")\n",
    "%autoreload 2\n",
    "\n",
    "# -- System and Path --\n",
    "import os\n",
    "import sys\n",
    "REPO_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.append(REPO_PATH)\n",
    "print(f\"REPO_PATH: {REPO_PATH}\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Imports --\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Configuration --\n",
    "class Config:\n",
    "    \"\"\"Holds all configuration parameters for the script.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.repo_path = os.path.abspath(os.path.join(\"..\"))\n",
    "        self.data_dir = os.path.join(self.repo_path, \"data\", \"thaisum\", \"raw\")\n",
    "        self.tokenized_data_dir = os.path.join(self.repo_path, \"data\", \"thaisum\", \"tokenized\")\n",
    "        self.model_name = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "        self.batch_size = 2\n",
    "        self.lr = 1e-5\n",
    "        self.num_epochs = 1\n",
    "        self.seed = 42\n",
    "        self.mask_token_id = 126336\n",
    "        self.device = self._select_device()\n",
    "\n",
    "    def _select_device(self):\n",
    "        \"\"\"Selects the best available device (CUDA, MPS, or CPU).\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        return device\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "def load_dataset_from_csv(train_file: str = None, valid_file: str = None, test_file: str = None, sample_size: int = None) -> DatasetDict:\n",
    "    \"\"\"Loads dataset splits from CSV files and optionally samples rows.\"\"\"\n",
    "    split_files = {\"train\": train_file, \"validation\": valid_file, \"test\": test_file}\n",
    "    dct = {}\n",
    "    for split, file_path in split_files.items():\n",
    "        if file_path:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if sample_size:\n",
    "                df = df.sample(sample_size)\n",
    "            dct[split] = Dataset.from_pandas(df)\n",
    "    return DatasetDict(dct)\n",
    "\n",
    "def format_llada_prompt(example, tokenizer):\n",
    "    \"\"\"Formats an example into a prompt for the LLaDA model and tokenizes it.\"\"\"\n",
    "    instruction = f\"<start_id>user<end_id>\\n{example['body']}<eot_id><start_id>assistant<end_id>\\n{example['summary']}<EOS>\"\n",
    "    tokenized = tokenizer(instruction, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    prompt_end = instruction.find(\"<start_id>assistant<end_id>\")\n",
    "    prompt_tokens = tokenizer(instruction[:prompt_end])[\"input_ids\"]\n",
    "    return {\"input_ids\": tokenized[\"input_ids\"], \"prompt_length\": len(prompt_tokens)}\n",
    "\n",
    "def load_and_preprocess_data(config: Config, sample_size: int = 100) -> tuple:\n",
    "    \"\"\"Loads and preprocesses the dataset, saving tokenized data to disk.\"\"\"\n",
    "    train_data_file = os.path.join(config.data_dir, \"train.csv\")\n",
    "    dataset_dict = load_dataset_from_csv(train_file=train_data_file, sample_size=sample_size)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "\n",
    "    os.makedirs(config.tokenized_data_dir, exist_ok=True)\n",
    "    processed_data = dataset_dict[\"train\"].map(lambda x: format_llada_prompt(x, tokenizer))\n",
    "    output_path = os.path.join(config.tokenized_data_dir, \"train.jsonl\")\n",
    "    processed_data.to_json(output_path)\n",
    "    print(f\"Saved tokenized data to: {output_path}\")\n",
    "    return processed_data, tokenizer\n",
    "\n",
    "# Model Loading\n",
    "def load_model(config: Config) -> AutoModel:\n",
    "    \"\"\"Loads the LLaDA model and prepares it for training.\"\"\"\n",
    "    print(f\"Loading {config.model_name} model...\")\n",
    "    model = AutoModel.from_pretrained(config.model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "    model.to(config.device)\n",
    "    model.train()\n",
    "    print(f\"{config.model_name} model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "# DataLoader Collate Function\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Prepares a batch for training by converting to tensors.\"\"\"\n",
    "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n",
    "    prompt_lengths = torch.tensor([item[\"prompt_length\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"prompt_lengths\": prompt_lengths}\n",
    "\n",
    "# Training Function\n",
    "def train_model(model: AutoModel, dataloader: DataLoader, optimizer: AdamW, num_epochs: int, device: str, mask_token_id: int):\n",
    "    \"\"\"Trains the model using a masked language modeling approach.\"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            prompt_lengths = batch[\"prompt_lengths\"].to(device)\n",
    "\n",
    "            # Create noisy batch by masking post-prompt tokens\n",
    "            noisy_batch = input_ids.clone()\n",
    "            for i in range(noisy_batch.shape[0]):\n",
    "                noisy_batch[i, prompt_lengths[i]:] = mask_token_id\n",
    "            mask_index = (noisy_batch == mask_token_id)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_ids=noisy_batch).logits\n",
    "            p_mask = torch.ones_like(noisy_batch, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Compute loss only on masked tokens\n",
    "            token_loss = F.cross_entropy(logits[mask_index], input_ids[mask_index], reduction=\"none\") / p_mask[mask_index]\n",
    "            loss = token_loss.sum() / input_ids.shape[0]\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "processed_data, tokenizer = load_and_preprocess_data(config, sample_size=100)\n",
    "# Load model\n",
    "model = load_model(config)\n",
    "# Prepare DataLoader\n",
    "dataloader = DataLoader(\n",
    "    processed_data, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "# Train model\n",
    "optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "train_model(\n",
    "    model, dataloader, optimizer, config.num_epochs, config.device, config.mask_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Functions\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    \"\"\"Adds Gumbel noise to logits for sampling.\"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    \"\"\"Calculates the number of tokens to transfer at each generation step.\"\"\"\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    return num_transfer_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    \"\"\"Generates text using the trained model.\"\"\"\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "    assert steps % num_blocks == 0\n",
    "    steps_per_block = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        start_idx = prompt.shape[1] + num_block * block_length\n",
    "        end_idx = start_idx + block_length\n",
    "        block_mask_index = (x[:, start_idx:end_idx] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n",
    "\n",
    "        for i in range(steps_per_block):\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "                x0_p = torch.squeeze(torch.gather(p, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand_like(x0, device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Remasking strategy '{remasking}' not implemented.\")\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Generate summary\n",
    "prompt_text = (\n",
    "    \"Summarize following text in less than 3 sentences: ความเก่ง เกิดขึ้นได้หลายแบบไม่ว่าจะ \"\n",
    "    \"ความหมั่นเพียร(ฝึกซ้อม), ประสบการณ์, สิ่งแวดล้อมเกื้อหนุน, มีต้นทุนบางอย่างดี \"\n",
    "    \"เหมือนคนเกิดมาร่างกายสูงใหญ่มีโอกาสเก่งในกีฬาหลายประเภท นี่ก็ถือว่าต้นทุนดี \"\n",
    "    \"แต่เหล่านี้เองจึงย้อนไปบั่นทอนคนที่คิดว่าตนไม่เก่ง เช่น เราขี้เกียจ-ไม่มีเวลาซ้อม, \"\n",
    "    \"เราไม่เคยทำมาก่อน, ยังไม่พร้อม, ต้นทุนไม่ดีเหมือนเขา ส่วนหนึ่งก็ใช่ว่าผิด \"\n",
    "    \"แต่แน่นอนไม่ถูก และกลายเป็นถ่วงอนาคตอย่างมาก\"\n",
    ")\n",
    "messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False\n",
    ")\n",
    "input_ids = torch.tensor(tokenizer(prompt)[\"input_ids\"]).to(config.device).unsqueeze(0)\n",
    "output_ids = generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    steps=128,\n",
    "    gen_length=128,\n",
    "    block_length=32,\n",
    "    temperature=0.0,\n",
    "    cfg_scale=0.0,\n",
    "    remasking=\"low_confidence\",\n",
    "    mask_id=config.mask_token_id,\n",
    ")\n",
    "summary = tokenizer.batch_decode(\n",
    "    output_ids[:, input_ids.shape[1] :], skip_special_tokens=True\n",
    ")[0]\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
