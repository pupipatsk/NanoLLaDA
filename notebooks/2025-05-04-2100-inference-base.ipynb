{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas torch tqdm transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- iPython Config --\n",
    "from IPython import get_ipython\n",
    "if \"IPython.extensions.autoreload\" not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")\n",
    "else:\n",
    "    get_ipython().run_line_magic(\"reload_ext\", \"autoreload\")\n",
    "%autoreload 2\n",
    "\n",
    "# -- System and Path --\n",
    "import os\n",
    "import sys\n",
    "REPO_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.append(REPO_PATH)\n",
    "print(f\"REPO_PATH: {REPO_PATH}\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Imports --\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Configuration --\n",
    "class Config:\n",
    "    \"\"\"Holds all configuration parameters for the script.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.repo_path = os.path.abspath(os.path.join(\"..\"))\n",
    "        self.data_dir = self.repo_path\n",
    "        self.tokenized_data_dir = os.path.join(self.repo_path, \"tokenized\")\n",
    "        self.model_name = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "        self.batch_size = 2\n",
    "        self.lr = 1e-5\n",
    "        self.num_epochs = 1\n",
    "        self.seed = 42\n",
    "        self.mask_token_id = 126336\n",
    "        self.device = self._select_device()\n",
    "\n",
    "    def _select_device(self):\n",
    "        \"\"\"Selects the best available device (CUDA, MPS, or CPU).\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        return device\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Generation Helper Functions --\n",
    "def add_gumbel_noise(logits, temperature):\n",
    "    \"\"\"Adds Gumbel noise to logits for sampling.\"\"\"\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (-torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    \"\"\"Calculates the number of tokens to transfer at each generation step.\"\"\"\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "    return num_transfer_tokens\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    \"\"\"Generates text using the trained model.\"\"\"\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "    assert steps % num_blocks == 0\n",
    "    steps_per_block = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):\n",
    "        start_idx = prompt.shape[1] + num_block * block_length\n",
    "        end_idx = start_idx + block_length\n",
    "        block_mask_index = (x[:, start_idx:end_idx] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)\n",
    "\n",
    "        for i in range(steps_per_block):\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature)\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1)\n",
    "\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits.to(torch.float64), dim=-1)\n",
    "                x0_p = torch.squeeze(torch.gather(p, dim=-1, index=x0.unsqueeze(-1)), -1)\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand_like(x0, device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Remasking strategy '{remasking}' not implemented.\")\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\n",
    "                transfer_index[j, select_index] = True\n",
    "            x[transfer_index] = x0[transfer_index]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load test data --\n",
    "df_test = pd.read_csv('/Users/pupipatsingkhorn/Developer/repositories/NanoLLaDA/data/test-100-1024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(config.model_name, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "model.to(config.device)\n",
    "model.eval()\n",
    "\n",
    "# Function to generate summary for a single input text\n",
    "@torch.no_grad()\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"สรุปข้อความต่อไปนี้\\n{text}\"}]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "        input_ids = torch.tensor(tokenizer(prompt)[\"input_ids\"]).to(config.device).unsqueeze(0)\n",
    "\n",
    "        output_ids = generate(\n",
    "            model,\n",
    "            input_ids,\n",
    "            steps=256,\n",
    "            gen_length=256,\n",
    "            block_length=32,\n",
    "            temperature=0.7,\n",
    "            cfg_scale=0.0,\n",
    "            remasking=\"low_confidence\",\n",
    "            mask_id=config.mask_token_id,\n",
    "        )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(\n",
    "            output_ids[:, input_ids.shape[1]:], skip_special_tokens=True\n",
    "        )[0]\n",
    "        return decoded.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "tqdm.pandas()\n",
    "df_test['generated'] = df_test['body'].progress_apply(summarize_text)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
