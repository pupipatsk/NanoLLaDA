{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- iPython Config --- #\n",
    "from IPython import get_ipython\n",
    "if 'IPython.extensions.autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "else:\n",
    "    get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "%autoreload 2\n",
    "\n",
    "# --- System and Path --- #\n",
    "import os\n",
    "import sys\n",
    "REPO_PATH = os.path.abspath(os.path.join('..'))\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.append(REPO_PATH)\n",
    "print(f\"REPO_PATH: {REPO_PATH}\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports --- #\n",
    "import pandas as pd\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "train_path = \"/path/to/train.csv\"\n",
    "valid_path = \"/path/to/valid.csv\"\n",
    "test_path = \"/path/to/test.csv\"\n",
    "output_dir = \"/mnt/data/thaisum_finetune\"\n",
    "\n",
    "# ==== Load tokenizer ====\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GSAI-ML/LLaDA-8B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "# ==== Load CSVs ====\n",
    "train_df = pd.read_csv(train_path)\n",
    "valid_df = pd.read_csv(valid_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# ==== Wrap in HuggingFace Datasets ====\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "# ==== Formatting function ====\n",
    "def format_llada_prompt(example):\n",
    "    instruction = f\"<start_id>user<end_id>\\n{example['body']}<eot_id><start_id>assistant<end_id>\\n{example['summary']}<EOS>\"\n",
    "    tokenized = tokenizer(instruction, padding=\"max_length\", truncation=True, max_length=1024)\n",
    "    prompt_end = instruction.find(\"<start_id>assistant<end_id>\")\n",
    "    prompt_tokens = tokenizer(instruction[:prompt_end])[\"input_ids\"]\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"prompt_length\": len(prompt_tokens)\n",
    "    }\n",
    "\n",
    "# ==== Process and save ====\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for split in dataset:\n",
    "    print(f\"Processing {split} split...\")\n",
    "    processed = dataset[split].map(format_llada_prompt)\n",
    "    output_path = os.path.join(output_dir, f\"{split}.jsonl\")\n",
    "    processed.to_json(output_path)\n",
    "    print(f\"âœ… Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==== CONFIG ====\n",
    "train_file = \"/mnt/data/thaisum_finetune/train.jsonl\"\n",
    "model_name = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "mask_token_id = 126336\n",
    "batch_size = 2\n",
    "lr = 1e-5\n",
    "num_epochs = 1  # Adjust as needed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ==== Load dataset ====\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": train_file})[\"train\"]\n",
    "\n",
    "# ==== Load tokenizer/model ====\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.bfloat16).to(device)\n",
    "model.train()\n",
    "\n",
    "# ==== Collate function ====\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch])\n",
    "    prompt_lengths = torch.tensor([item[\"prompt_length\"] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"prompt_lengths\": prompt_lengths}\n",
    "\n",
    "# ==== DataLoader ====\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# ==== Optimizer ====\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# ==== Training loop ====\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        prompt_lengths = batch[\"prompt_lengths\"].to(device)\n",
    "\n",
    "        # Mask everything except the prompt\n",
    "        noisy_batch = input_ids.clone()\n",
    "        for i in range(noisy_batch.shape[0]):\n",
    "            noisy_batch[i, prompt_lengths[i]:] = mask_token_id\n",
    "\n",
    "        mask_index = (noisy_batch == mask_token_id)\n",
    "\n",
    "        logits = model(input_ids=noisy_batch).logits\n",
    "        p_mask = torch.ones_like(noisy_batch, dtype=torch.float32).to(device)\n",
    "\n",
    "        token_loss = F.cross_entropy(\n",
    "            logits[mask_index], input_ids[mask_index], reduction=\"none\"\n",
    "        ) / p_mask[mask_index]\n",
    "\n",
    "        loss = token_loss.sum() / input_ids.shape[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
